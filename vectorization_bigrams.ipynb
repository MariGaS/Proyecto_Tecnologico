{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariGaS/Proyecto_Tecnologico/blob/main/vectorization_bigrams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oB0A3cZw9mn"
      },
      "outputs": [],
      "source": [
        "import nltk \n",
        "from nltk import TweetTokenizer\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import re\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support, roc_auc_score\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQRs3ngz4xBp"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "import collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nOtK4G1yd2Z"
      },
      "outputs": [],
      "source": [
        "def get_urls(path): \n",
        "    onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
        "    onlyfiles.sort()\n",
        "\n",
        "    return onlyfiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDwyk9bgygMr"
      },
      "outputs": [],
      "source": [
        "def get_text_post_train(user_path):\n",
        "    tree = ET.parse(user_path)\n",
        "    root = tree.getroot()\n",
        "    hist_post = [] # list with entries from the user \n",
        "\n",
        "    #iterate recursively over all the sub-tree \n",
        "    #source : https://docs.python.org/3/library/xml.etree.elementtree.html\n",
        "    for post in root.iter('WRITING'): \n",
        "\n",
        "        for t in post.iter('TITLE'):\n",
        "            entry = t.text\n",
        "            entry = entry.replace('\\n', ' ') \n",
        "            if entry == ' ' or entry == '  ':\n",
        "                continue \n",
        "            else: \n",
        "                hist_post.append(entry[1:-1])\n",
        "        for t in post.iter('TEXT'):\n",
        "            entry = t.text\n",
        "            entry = entry.replace('\\n', ' ') \n",
        "            if entry == ' ' or entry == '  ':\n",
        "                continue \n",
        "            else: \n",
        "                hist_post.append(entry[1:-1])\n",
        "\n",
        "    \n",
        "    ##--------concatenate post -------------#\n",
        "    all_post = \"\" #concatenate all post \n",
        "    for i in range(len(hist_post)):\n",
        "        all_post = all_post + hist_post[i] + \" \"\n",
        "    \n",
        "    return all_post "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVBfiR7uyjFe"
      },
      "outputs": [],
      "source": [
        "def get_text_labels(path, polarity = 'Negative'): \n",
        "    all_documents = [] #list with all the documents \n",
        "    all_label = [] #label \n",
        "\n",
        "    user_path = get_urls(path)\n",
        "    for i in range(len(user_path)): \n",
        "        subject = user_path[i] #for example test_subjet1005.xml \n",
        "        path_subject = path + '/' + subject \n",
        "        document = get_text_post_train(path_subject) #get document with all the history of a user \n",
        "\n",
        "        all_documents += [document] \n",
        "        if polarity == 'Negative' : \n",
        "            all_label += [0]\n",
        "        else: \n",
        "            all_label += [1]\n",
        "\n",
        "    return all_documents, all_label \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xjvcvjYylAv"
      },
      "outputs": [],
      "source": [
        "def get_text_chunk(path): \n",
        "    all_documents = [] #list with all the documents \n",
        "    all_label = [] #label \n",
        "\n",
        "    user_path = get_urls(path)\n",
        "    for i in range(len(user_path)): \n",
        "        subject = user_path[i] #for example test_subjet1005.xml \n",
        "        path_subject = path + '/' + subject \n",
        "        document = get_text_post_train(path_subject) #get document with all the history of a user \n",
        "\n",
        "        all_documents += [document] \n",
        "    return all_documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SEqNp7fAE93"
      },
      "source": [
        "# Extraer train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq3ZorjmpmVn"
      },
      "outputs": [],
      "source": [
        "anxia_test = '/content/drive/MyDrive/Proyecto_Tecnologico/Data/Anorexia Datasets_1/test'\n",
        "anxia_train = '/content/drive/MyDrive/Proyecto_Tecnologico/Data/Anorexia Datasets_1/train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_tI_bRaKzoy"
      },
      "outputs": [],
      "source": [
        "pos = 'positive_examples'\n",
        "neg = 'negative_examples'\n",
        "\n",
        "all_pos = []\n",
        "all_neg = []\n",
        "for i in range(1,11):\n",
        "    path_chunk_pos = anxia_train +'/' + pos + '/chunk'+str(i)\n",
        "    path_chunk_neg = anxia_train +'/' + neg + '/chunk'+str(i)\n",
        "\n",
        "    temp1 = get_text_chunk(path_chunk_pos)\n",
        "    temp2 = get_text_chunk(path_chunk_neg)\n",
        "    if i == 1: \n",
        "        all_pos = temp1\n",
        "        all_neg = temp2\n",
        "    else: \n",
        "        for j in range(len(temp1)):\n",
        "            all_pos[j] += temp1[j]\n",
        "    \n",
        "        for j in range(len(temp2)):\n",
        "            all_neg[j] += temp2[j]\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNzt4S_kNgdY"
      },
      "outputs": [],
      "source": [
        "tr_anorexia = [*all_pos, *all_neg]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62f-s4rvN9B4"
      },
      "outputs": [],
      "source": [
        "tr_label = []\n",
        "for i in range(len(tr_anorexia)):\n",
        "    if i < 20: \n",
        "        tr_label.append(1)\n",
        "    else:\n",
        "        tr_label.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "gfibKDKlbbjO",
        "outputId": "3b002cb6-6fc5-454e-ac4a-adc3ba02339c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({0: 132, 1: 20})\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "print(Counter(tr_label))\n",
        "plt.hist(tr_label, bins=len(set(tr_label)))\n",
        "plt.ylabel('Users');\n",
        "plt.xlabel('Class');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dip9D0pfyuM_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jNMMSNoWRoh"
      },
      "source": [
        "# Test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiAAqvXGWUDo"
      },
      "outputs": [],
      "source": [
        "test_url_anxia   = []\n",
        "test_labels_anxia = []\n",
        "\n",
        "with open('/content/drive/MyDrive/Proyecto_Tecnologico/Data/Anorexia Datasets_1/test/test_golden_truth.txt') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines: \n",
        "\n",
        "        test_url_anxia.append(line[:-3]) #only the name of the subject \n",
        "\n",
        "        test_labels_anxia.append(int(line[-2:-1])) #only the label\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-utl3gfWX1-"
      },
      "outputs": [],
      "source": [
        "def get_text_test_anorexia(path, list_subject, chunk): \n",
        "    all_documents = [] #list with all the documents \n",
        "    all_label = [] #label \n",
        "\n",
        "    for i in range(len(list_subject)): \n",
        "        subject = list_subject[i] #for example test_subjet1005.xml \n",
        "        path_subject = path + '/chunk'+ str(chunk) + '/' + subject + '_' + str(chunk) + '.xml'\n",
        "        \n",
        "        document = get_text_post_train(path_subject) #get document with all the history of a user \n",
        "        \n",
        "        all_documents += [document] \n",
        "    print(\"text extracted for all users\")\n",
        "    return all_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxjZaMajWYqE",
        "outputId": "6a24a0d0-8751-409c-9d87-5a8ee2582748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text extracted for all users\n",
            "Text extracted from chunk:  1\n",
            "text extracted for all users\n",
            "Text extracted from chunk:  2\n",
            "text extracted for all users\n",
            "Text extracted from chunk:  3\n",
            "text extracted for all users\n",
            "Text extracted from chunk:  4\n",
            "text extracted for all users\n",
            "Text extracted from chunk:  5\n",
            "text extracted for all users\n",
            "Text extracted from chunk:  6\n",
            "text extracted for all users\n",
            "Text extracted from chunk:  7\n",
            "text extracted for all users\n",
            "Text extracted from chunk:  8\n",
            "text extracted for all users\n",
            "Text extracted from chunk:  9\n",
            "text extracted for all users\n",
            "Text extracted from chunk:  10\n"
          ]
        }
      ],
      "source": [
        "test_anxia = []\n",
        "for i in range(1,11):\n",
        "\n",
        "    temp1 = get_text_test_anorexia(anxia_test, test_url_anxia, i)\n",
        "\n",
        "    if i == 1: \n",
        "        test_anxia = temp1\n",
        "        print(\"Text extracted from chunk: \", i)\n",
        "    else: \n",
        "\n",
        "        for j in range(len(temp1)):\n",
        "            test_anxia[j] += temp1[j]\n",
        "        print(\"Text extracted from chunk: \", i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnuPZwaLy0nm"
      },
      "outputs": [],
      "source": [
        "def normalize(document): \n",
        "\n",
        "    document = [x.lower()  for x in document]\n",
        "    document = [re.sub(r'https?:\\/\\/\\S+', '', x) for x in document] #eliminate url\n",
        "    document = [re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x) for x in document] #eliminate url \n",
        "    document = [re.sub(r'{link}', '', x) for x in document] #eliminate link\n",
        "    document = [re.sub(r\"\\[video\\]\", '', x) for x in document] #eliminate video \n",
        "    document = [re.sub(r'\\s+', ' ' '', x).strip() for x in document]\n",
        "    document = [re.sub(r',', ' ' '', x).strip() for x in document]\n",
        "    document = [x.replace(\"#\",\"\") for x in document]  #eliminate #\n",
        "    document = [re.subn(r'[^\\w\\s,]',\"\", x)[0].strip() for x in document] #eliminate emoticons \n",
        "\n",
        "    return document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bk2UhbH4Ge2"
      },
      "outputs": [],
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyDDksedy3uS"
      },
      "outputs": [],
      "source": [
        "def remove_stop_list(document):\n",
        "    document = [remove_stopwords(x)  for x in document]\n",
        "\n",
        "    return document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfPx5yEc4Dpk"
      },
      "outputs": [],
      "source": [
        "data_pos = remove_stop_list(all_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk2xrEUQ4LKm"
      },
      "outputs": [],
      "source": [
        "data_pos = normalize(data_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3fUhCxo4f_1"
      },
      "outputs": [],
      "source": [
        "tokenizer = TweetTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6QynzB-4Wgb"
      },
      "outputs": [],
      "source": [
        "def get_corpus(text):\n",
        "    corpus_palabras = []\n",
        "    for doc in text: \n",
        "        corpus_palabras += tokenizer.tokenize(doc)\n",
        "    return corpus_palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWDipDBO4sG8"
      },
      "outputs": [],
      "source": [
        "c = get_corpus(data_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zErHWXQ4yHR"
      },
      "outputs": [],
      "source": [
        "esBigrams = ngrams(c, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXCE_siY5Fdh"
      },
      "outputs": [],
      "source": [
        "# get the frequency of each bigram in our corpus\n",
        "#https://www.kaggle.com/code/rtatman/tutorial-getting-n-grams \n",
        "esBigramFreq = collections.Counter(esBigrams)\n",
        "\n",
        "# what are the ten most popular ngrams in this Spanish corpus?\n",
        "l = esBigramFreq.most_common(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mO2kJLCDUQ5"
      },
      "outputs": [],
      "source": [
        "def get_bigrams(data, num_bigrams):\n",
        "    corpus = get_corpus(data)\n",
        "    esBigrams = ngrams(corpus, 2)\n",
        "    esBigramFreq = collections.Counter(esBigrams)\n",
        "    l = esBigramFreq.most_common(num_bigrams)\n",
        "\n",
        "    bigrams = []\n",
        "    for i in range(len(l)):\n",
        "        w1 = l[i][0][0]\n",
        "        w2 = l[i][0][1]\n",
        "\n",
        "        w = w1 + '_' + w2\n",
        "        bigrams.append(w)\n",
        "    \n",
        "    return bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRoog3rZFksX"
      },
      "outputs": [],
      "source": [
        "def get_words(fdist_doc):\n",
        "    '''\n",
        "    For each user we have the dictionary of frequency of their words\n",
        "    we have a reduced list by check is their words have a representation\n",
        "    in GloVe\n",
        "\n",
        "    Return: List\n",
        "    '''\n",
        "    words_doc = []\n",
        "    for i,word in fdist_doc:\n",
        "        #if word.isnumeric() == False:\n",
        "            #embeddings_vector = model.wv[word]\n",
        "            #if embeddings_vector is not None: \n",
        "        words_doc.append(word)\n",
        "    \n",
        "    return words_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7mVJcmiDIp0"
      },
      "outputs": [],
      "source": [
        "def get_dic_bigram(fdist1, bigrams):\n",
        "    words1 = get_words(fdist1)\n",
        "    words = words1 + bigrams\n",
        "    vocab_plus = set(words)\n",
        "    #vocab_emb = get_words_dic(vocab_plus)\n",
        "    value = [x for x in range(len(vocab_plus))] \n",
        "    dictionary = dict(zip(value, words)) # nuevo diccionario \n",
        "\n",
        "    return dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LEFJ8ixF30h"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "model = FastText.load('/content/drive/MyDrive/Proyecto_Tecnologico/Model/fasttext_anxiety.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-vNVy2jKFRU"
      },
      "outputs": [],
      "source": [
        "def get_fuzzy_rep(words_doc, dictionary, epsilon):\n",
        "    words_user = np.zeros((len(words_doc),300) , dtype=float)\n",
        "    dictionary_vec = np.zeros((len(set(dictionary)),300) ,dtype=float)\n",
        "\n",
        "    for i in range(words_user.shape[0]): \n",
        "        w1 = words_doc[i]         \n",
        "        words_user[i] = model.wv[w1]\n",
        "\n",
        "    for i in range(dictionary_vec.shape[0]): \n",
        "        w1 = dictionary[i]         \n",
        "        dictionary_vec[i] = model.wv[w1]\n",
        "\n",
        "    similarity_vocab = sklearn.metrics.pairwise.cosine_similarity(words_user, dictionary_vec)\n",
        "    #vector de representación\n",
        "    vec_representation = np.count_nonzero(similarity_vocab > epsilon , axis=0)\n",
        "\n",
        "     \n",
        "    return vec_representation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag7tKmSWGACB"
      },
      "outputs": [],
      "source": [
        "def sortFreqDict(freqdict):\n",
        "        aux = [(freqdict[key],key) for key in freqdict]\n",
        "        aux.sort()\n",
        "        aux.reverse()\n",
        "        return aux "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sl6LNsTQGtv8"
      },
      "outputs": [],
      "source": [
        "def get_fdist(text, num_feat):\n",
        "    corpus_palabras = []\n",
        "    for doc in text: \n",
        "        corpus_palabras += tokenizer.tokenize(doc)\n",
        "    fdist = nltk.FreqDist(corpus_palabras)\n",
        "    fdist = sortFreqDict(fdist)\n",
        "    fdist = fdist[:num_feat]\n",
        "    return fdist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh3SCwo4InQZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def classificator_pos_neg(pos_data, neg_data, test, num_feat, num_bigrams, tau, remove_stop = False):\n",
        "    if remove_stop == True: \n",
        "        print(\"quitando stopwords\")\n",
        "        pos_data = remove_stop_list(pos_data)\n",
        "        neg_data = remove_stop_list(neg_data)\n",
        "        test     = remove_stop_list(test)\n",
        "\n",
        "    pos_data = normalize(pos_data)\n",
        "    neg_data = normalize(neg_data)\n",
        "    test = normalize(test)\n",
        "\n",
        "\n",
        "    num_test = len(test)\n",
        "\n",
        "    fdist_pos = get_fdist(pos_data, num_feat)\n",
        "    fdist_neg = get_fdist(neg_data, num_feat)\n",
        "\n",
        "    bigrams1 = get_bigrams(pos_data, num_bigrams)\n",
        "    bigrams2 = get_bigrams(neg_data, num_bigrams)\n",
        "    dictionary1 = get_dic_bigram(fdist_pos, bigrams1)\n",
        "    dictionary2 = get_dic_bigram(fdist_neg, bigrams2)\n",
        "    \n",
        "    \n",
        "    X_test1 = np.zeros((num_test,len(dictionary1)) ,dtype=float) #matriz tipo document-term\n",
        "    X_test2 = np.zeros((num_test,len(dictionary2)) ,dtype=float)\n",
        "\n",
        "    print(\"inicio vectorizacion con pos \")\n",
        "    for i in range(num_test): \n",
        "        doc = test[i]\n",
        "        corpus_palabras = tokenizer.tokenize(doc.lower())\n",
        "        fdist = nltk.FreqDist(corpus_palabras)\n",
        "        v = sortFreqDict(fdist)\n",
        "        words_doc =  get_words(v) \n",
        "        bigrams_test1 = get_bigrams(doc, num_bigrams)\n",
        "        words_doc = words_doc + bigrams_test1\n",
        "        word_repre_user = get_fuzzy_rep(words_doc, dictionary1, epsilon = tau)\n",
        "        X_test1[i] = word_repre_user\n",
        "    print('end')\n",
        "    print(\"inicio vectorizacion con neg\")\n",
        "    for i in range(num_test): \n",
        "        doc = test[i]\n",
        "        corpus_palabras = tokenizer.tokenize(doc.lower())\n",
        "        fdist = nltk.FreqDist(corpus_palabras)\n",
        "        v = sortFreqDict(fdist)\n",
        "        words_doc =  get_words(v) \n",
        "        bigrams_test2 = get_bigrams(doc, num_bigrams)\n",
        "        words_doc = words_doc + bigrams_test2\n",
        "        word_repre_user = get_fuzzy_rep(words_doc, dictionary2, epsilon = tau)\n",
        "        X_test2[i] = word_repre_user\n",
        "    #print(\"Vectorization for test data: Done\")\n",
        "    print('end')\n",
        "    X_test1 = np.sum(X_test1, axis = 1)  \n",
        "    X_test2 = np.sum(X_test2, axis = 1)  \n",
        "\n",
        "    results = np.zeros((num_test), dtype = int)\n",
        "    for i in range(num_test):\n",
        "        if X_test1[i] > X_test2[i]:\n",
        "            results[i] = 1\n",
        "        else: \n",
        "            results[i] = 0\n",
        "    return results, X_test1, X_test2\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Be9GWN0HfBN",
        "outputId": "92b8e0c2-fe66-4c0f-b6d8-f947f1a139d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "quitando stopwords\n",
            "inicio vectorizacion con pos \n",
            "end\n",
            "inicio vectorizacion con neg\n",
            "end\n",
            "[[274   5]\n",
            " [ 13  28]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.98      0.97       279\n",
            "           1       0.85      0.68      0.76        41\n",
            "\n",
            "    accuracy                           0.94       320\n",
            "   macro avg       0.90      0.83      0.86       320\n",
            "weighted avg       0.94      0.94      0.94       320\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "results5, x, y = classificator_pos_neg(all_pos, all_neg, test_anxia, 500,50, 0.99, remove_stop= True)\n",
        "p , r, f, _ = precision_recall_fscore_support(test_labels_anxia, results5, average = 'macro', pos_label = 1)\n",
        "\n",
        "print(confusion_matrix(test_labels_anxia,results5))\n",
        "print(metrics.classification_report(test_labels_anxia,results5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGphjANsVALO",
        "outputId": "82bef9e9-d36c-4bef-851e-8ec847f5e8a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "quitando stopwords\n",
            "inicio vectorizacion con pos \n",
            "end\n",
            "inicio vectorizacion con neg\n",
            "end\n",
            "[[272   7]\n",
            " [ 15  26]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96       279\n",
            "           1       0.79      0.63      0.70        41\n",
            "\n",
            "    accuracy                           0.93       320\n",
            "   macro avg       0.87      0.80      0.83       320\n",
            "weighted avg       0.93      0.93      0.93       320\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "results5, x, y = classificator_pos_neg(all_pos, all_neg, test_anxia, 450,100, 0.99, remove_stop= True)\n",
        "p , r, f, _ = precision_recall_fscore_support(test_labels_anxia, results5, average = 'macro', pos_label = 1)\n",
        "\n",
        "print(confusion_matrix(test_labels_anxia,results5))\n",
        "print(metrics.classification_report(test_labels_anxia,results5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHeAVe-Wb8NU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "vectorization_bigrams",
      "provenance": [],
      "mount_file_id": "1hJ2r8DyJbBF6bzv2uwuJPEsDB5Okbg_k",
      "authorship_tag": "ABX9TyPRIFGvILiIuilKdmBtV+te",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}